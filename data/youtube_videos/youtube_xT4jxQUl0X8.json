{
  "title": "DeepSeek's GRPO (Group Relative Policy Optimization) | Reinforcement Learning for LLMs",
  "author": "Julia Turc",
  "video_id": "xT4jxQUl0X8",
  "source_url": "https://www.youtube.com/watch?v=xT4jxQUl0X8",
  "source_type": "youtube",
  "thumbnail_url": "https://i.ytimg.com/vi/xT4jxQUl0X8/hqdefault.jpg",
  "html": "<iframe width=\"200\" height=\"113\" src=\"https://www.youtube.com/embed/xT4jxQUl0X8?feature=oembed\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" referrerpolicy=\"strict-origin-when-cross-origin\" allowfullscreen title=\"DeepSeek&#39;s GRPO (Group Relative Policy Optimization) | Reinforcement Learning for LLMs\"></iframe>",
  "width": 200,
  "height": 113,
  "transcript": "grpo or group relative policy\noptimization is the core reinforcement\nlearning algorithm behind the Deep seek\nInnovation and the reason why they were\nallegedly able to train a reasoning\nmodel at a fraction of the cost normally\nincurred by open Ai and now it's\nstarting to get adopted by other llm\nproviders like quen now grpo itself is\njust a tweak to existing algorithms like\nopen A's po but it was built on a rich\nhistory of reinforcement learning\nresearch and that's why it might seem\nhard to digest at first my goal here is\nto provide you with enough context so\nthat you can understand what all the\nfuss is about and also to be able to\ntweak those annoying hyperparameters\nneeded when using the grpo trainer so\nwe're going to get started by\nunderstanding where grpo fits into the\nlarger llm training pipeline so we start\nwith a randomly initialized model that\njust outputs gibberish then we pre-train\nit by predicting the next token on the\nentirety of the internet implicitly the\nspace model ends up acquiring a lot of\nWorld Knowledge but it's not quite\nusable yet as an assistant because it\njust finishes your sentence and buils on\ntop of it to make it more conversational\nwe run instruction funing using\ninstruction response bears at this point\nwe've exhausted the entirety of the\ninternet and potentially our budget for\nhuman label data as well so in order to\nmake further improvements we'll have to\nbe more creative with the type of data\nthat we use this is where reinforcement\nlearning comes in it helps us train\nmodels without explicit reference\nanswers and use implicit signals instead\none such type of signal is human\npreference you might be familiar with\nthis chat GPT interface where it asks\nyou to choose between two alternative\nresponses open ai's goal here is to\ngather user preferences and fine-tune\ntheir models in a way that better aligns\nwith them this process is generally\ncalled preference fine tuning and there\nare multiple implementations notably\nopen AI introduced rhf or reinforcement\nlearning with human feedback this is\nquite an elaborate method that requires\ntraining an additional reward model\nwhich can predict human preferences and\nthe core training algorithm that updates\nthe model weights based on the human\npreference is called po or proximal\npolicy optimization the reason why I'm\nmentioning this implementation detail is\nbecause grpo which is the topic of this\nvideo is a successor of Po and I will\noccasionally reference po here but if\nyou want to dive deeper check out my\nprevious video a more recent and\ncomputationally efficient alternative to\nrhf is DPO or direct preference\noptimization we don't know if open AI\nswitched to DPO but the open source\nCommunity has definitely embraced it to\nlearn how DPO Works check out my other\nvideo on Tulu 3 an open source model\nthat uses it so that was preference\nfine-tuning but there's another type of\nimplicit signal that we can leverage to\nmake llms even better there's a special\nclass of problems called reasoning tasks\nwhich includes math computer science\nlogic and so on as opposed to more\ncreative tasks there's usually one right\nanswer and a deterministic way of\nfiguring out whether the model was\ncorrect so we can hardness this binary\ncorrectness signal for reasoning\nfine-tuning Under the Umbrella of\nreasoning F tuning you'll hear about the\nrvr algorithm or reinforcement learning\nwith verifiable rewards it's likely that\nopen AI has been using something similar\nto rvr for a while at least is their\nfirst reasoning model 01 which they\nannounced back in September 2024 but\ndeeps brought it more prominently to our\nattention when their open source R1\nmodel turned out to be a serious\ncompetitor that was allegedly trained\nmuch more cheaply at its core rvr also\nneeds an algorithm that updates the\nmodel weights based on the correctness\nsignal for this purpose we could use\nopen A's poo which we already\nencountered in the context of rhf or\ndeep seeks grpo which is a much more\nefficient alternative so hopefully this\ngives you an idea of where grpo fits\ninto the llm training pipeline it should\nbe clear by now that grpo sits at the\ncore of reinforcement learning so I'll\nspend the next chapter talking about\ncore building blocks of reinforcement\nlearning but if you're already familiar\nwith these Concepts or if you watched my\nprevious Bo video feel free to skip it\nreinforcement learning is much more\nintuitive in the physical world where we\nhave an agent like a Roomba navigating\nan environment like your living room\nwith the goal of sweeping the entire\nsurface at each time step T the agent\ntakes an action a index T like moving\none foot to the right which takes it to\na new state s index T this could be a\nnew XY position or the agent could get\nstuck under your desk the environment\ncan also communicate a reward R and XT\nwhich reflects how much progress that\nrobot made towards sweeping the entirety\nof the living room finally the\ntrajectory tow consists of all the\nstates the agent visited and all the\nactions it performed during one episode\nor one cleaning session in this case in\nthe language world the agent is an llm\nlike llama and the environment is the\nworld external to the llm including\nhumans data sets in tools like a python\ninterpreter to discuss what the actions\nand states are we'll walk together\nthrough a math question which we would\nuse for reasoning fine-tuning let's take\nfor instance this middle school math\nproblem for which 72 is the correct\nanswer when passing the question to the\nmodel we can use heuristics to\ndeterministically verify that its answer\nis 72 matches our reference answer when\nasking this math question I basically\nset the agent to its initial State as\nzero which consists of my prompt and the\nagent took its first action a z and\npredicted the token two the environment\nupdated the state to S1 in order to\nreflect this newly predicted token and\nthe agent proceeded with action A1 which\nwas to predict the token find and the\nenvironment added it to the state the\nmodel's next action was to predict out\nand you get the point this continues\nuntil the agent predicts the end token\nand concludes the episode so these were\nthe states and actions but what about\nthe Rewards well given how we design our\ntask we can only judge the response\nquality at the very end of the response\nthe environment can use some heuristics\nto pull out the final answer which is 72\nand then compare it against the ground\ntruth which is also 72 and since the\nmodel was right it will get a positive\nreward I'm choosing here a reward of one\nbut you could set it to any positive\nvalue instead and you can also be more\ncreative with the reward for instance\ndeep seek scores the model response not\njust on correctness but also on\nformatting so to recap for our\nparticular setup all intermediate\nrewards are zero except for the very\nlast one but there are cases where\npeople use external reward models to\nprovide intermediate rewards at the end\nof each sentence in the response okay so\nthe big picture is the agent takes a\nseries of actions and navigates a series\nof states and receives a reward at the\nend of the episode but here's the tricky\nbit somehow based on this single numeric\nvalue we need to back propagate through\nthrough the entire trajectory and\nthrough the model and update the weights\nin a way that reinforces good actions\nthis is incredibly difficult and the\nreason why reinforcement learning has\nbeen an active area of research for\ndecades so let's revisit our state S2 to\ntalk about what exactly happens during\none training step but first just a\nlittle terminology the llm is also known\nas the policy of the agent and is\ndenoted by pi index Theta where Theta\nare the model parameters this is just\nthe new name for the same llm we've been\nusing so far so the agent uses its\npolicy to produce a probability\ndistribution over the next actions given\nthe current state this is simply saying\nwe're passing the instruction and the\ntokens generated so far to the llm which\ngives us a probability for each next\ntoken in the vocabulary and since the\ntoken out has high probability it ends\nup being sampled let's zoom into the\nprobability distribution output by our\npolicy given the we eventually get a\npositive reward for the full response we\nknow the action of predicting out was\ngood or at least not disastrous so after\none training step we want to increase\nthe probability that our policy will\nassign to the Token out in the state to\nfind and slightly decrease the\nprobability of all other tokens both\ngrpo and po are part of a family of\nreinforcement learning algorithms called\npolicy gradient methods they achieve the\ndesired effect of increasing the\nprobability of the token out given by pi\nTheta of A2 conditioned on state 2 by\nfollowing the policy gradient and let's\nvisualize what that means of course our\npolicy has billions of parameters which\nwe can't visualize as humans so for\nSimplicity let's assume our policy only\nhad two parameters Theta 1 and Theta 2\nso we'll plug the output of the policy\nPi Theta only in terms of these two\nlet's also assume that the policy space\nis this nice gaussian surface so when we\ninitialize the policy parameters we\nbasically pick a point in this space\nhere my toy initial parameter values are\n05 and minus one so the goal of one\ntraining step is to move this ball\ncloser to the peak of the mountain and\npolicy gradient methods are simply doing\ngradient Ascent on the policy surface\nthey do so by first calculating the\ngradient which is this Black Arrow and\ntaking a step in that direction here I\nactually took multiple steps just to\nconfirm we would eventually reach the\npeak formally this is how we update the\nmodel weights by going in the direction\nof the gradient just a few notes here\nfirst of all we sum over all the time\nsteps or old tokens in the response\nsecond we take the logarithm of the\npolicy probability just because it's\nmathematically more convenient but this\nwill still pull the ball in the same\ndirection and third note that I'm also\nmultiplying the gradient by the reward\nof the final response which I'm going to\nwrite as upper caps are so when the\nreward is positive we go up the mountain\nwhen it's zero we just don't make an\nupdate and when it's negative we go down\nthe mountain or Shrink the probability\nto be consistent with the policy\ngradient Expressions that you might find\nin books I wanted to come back to the\nreward for a math problem we only have a\nsingle final reward for the entire Model\nepisode but more generally we could have\nintermediate rewards if the environment\nused some sort of reward model to score\nat a sentence or even token level so\nthen for each action we could calculate\nsomething called the return denoted by G\nindex T which is simply the sum of\nfuture rewards for example the return\nfor Action T minus 2 is the sum of the\ntwo rewards R tus2 plus r tus1 it's also\ncommon to add a discount Factor gamma\nthis is a hyperparameter between 0 and\none whose purpose is to exponentially\nreduce the impact of future rewards so\nwhen calculating the return of one\naction the longer ahead we look into the\nfuture the less credit we take for\nrewards that are further away so if we\nsubstitute GT in our policy gradient\nupdate we basically reconstructed\nreinforce which is the very first policy\ngradient method that was published back\nin 1992 this is the precursor of modern\nalgorithms like grpo and po one\nvariation of of the reinforce algorithm\nis to subtract a baseline from the\nreturn so what's the Baseline for well\nlet me put it this way let's say the\nagent's goal is to become the richest\nperson on Earth and say that it succeeds\nhence it gets a positive reward the\nquestion is how good was the last action\nat minus one how much partial credit\nshould we give it for the agent\nachieving this goal well if the agent\nstarts off in the shoes of Jeff bezo\nthen it'll have some work to do but it\ndoesn't feel impossible however if the\nagent starts in my shoes then that\naction 18 minus one is starting to look\nlike black magic my action is a lot more\nmeaningful than the current day Jeff\nBezos is so the baseline or the place\nwhere we start from matters a lot when\njudging the Merit of the action that we\ntook and various algorithms estimate\nthis Baseline in various ways there's a\nwhole family of algorithms called actor\ncritic methods which also includes boo\nthat estimate the Baseline using a state\nvalue function denoted by V index 5 this\nis a separate model from the policy llm\nwhich comes with its own trainable\nweights in this context the policy is\nalso known as the actor because it\nenacts or it takes actions and this\nstate value function is known as the\ncritic because it's constantly\nevaluating the states the agent goes\nthrough adding the critic brings a lot\nof computational overhead first first\nbecause the critic is probably\ninitialized from a big model like an llm\nso it's doubling the memory requirements\nbut also the computational resources\nbecause now we need to train an\nadditional model but the biggest\ndisadvantage in my opinion is the\ncognitive load for practitioners in\nother words these algorithms are just\nharder to understand here's for instance\nan excerpt from a paper that rewrites\nthis Delta in terms of all future State\nvalues to dig deeper you can check out\nmy poo video by the way this Delta is\nalso known as the advantage and is\ndenoted by upper caps a index T after\nmany years in which actor critic models\nwere very fashionable grpo goes back to\nthe basics and adopts a very simple\nBaseline in fact it even stops Computing\ndifferent advantages for each time step\ninstead of a per step return it just\nuses the final reward and instead of a\nper state Baseline it just uses the same\nBaseline across all time steps which we\ncall beat this might look like a massive\noversimplification but I personally\nthink it makes a lot of sense in the\nlanguage domain where we don't usually\nget intermediate Rewards or if we do\nthey come from reward models which are\ninaccurate anyway in contrast when you\nthink of video games we're constantly\ngetting feedback from the environment be\nit being punched in the face or getting\na gold medal or whatever but in the lack\nof intermediate rewards are intermediate\nestimations of advantages and value\nstate States and so on are probably\nwrong anyway so grpo is just embracing\nthis reality let's see how grpo computes\nthe Baseline B for a given instruction\nhere we're looking at the full\ntrajectory generated by the agent in\nresponse to the instruction to compute a\nbaseline for their reward grpo samples\nan additional group of trajectories\nsomewhere between four and eight and\ncollects their rewards so the Baseline\nis simply the average reward of this\ngroup to rewrite this consistently with\nthe paper the advantage is the final\nreward minus the mean reward of the\ngroup but we also divide by the standard\ndeviation of the group which is a common\nway to normalize values as statistics\nnow it should be clear why this method\nis called group relative policy\noptimization okay so grpo makes a lot of\nsimplifying assumptions but to be fair\nresearchers at Deep seek did run\nexperiments with intermediate rewards\ngiven a model response they use\nteristics to separate into reasoning\nsteps roughly a reasoning step might\ncorrespond to a sentence so we have\nseven reasoning steps here then they\nadded a reward model to the environment\nwhich provides intermediate rewards for\neach of these steps they call this\nprocess supervision note it has nothing\nto do with supervised learning it's just\na confusing name for intermediate\nrewards for each sentence in the\nresponse the difference is at each time\nstep T will use the reward for the\nreasoning step that t is part of so for\ninstance all these tokens in April\nNatalia and so on all use the reward of\nthe second reasoning step and when it\ncomes to the Baseline the mean is over\nall reasoning steps in all groups so are\nthe intermediate rewards worth it this\nfigure Compares outcome Supervision in\nyellow which means a single final reward\nagainst process Supervision in blue\nwhich has intermediate rewards we're\nseeing a little bit of Delta here on the\nGSM 8K data set set which is a middle\nschool math questions data set but if\nyou ask me no these small gains don't\njustify the overhead it seems like the\nrecurring theme in the grpo research is\njust keep it simple now let's talk about\nthe grpo loss making our way backwards\nfrom the gradient the loss would look\nlike this I'll call it LPG for policy\ngradient this loss is fine after all\nthis is what the original reinforce\nalgorithm used but throughout the years\nwe've made a ser of improvements and the\nreason why we didn't settle for this\nexpression is a Perpetual problem in\nreinforcement learning called variance\nvariance happens when the model updates\nare too large and we're basically\nreading too much into some noisy aspect\nof the data if we're not careful with\nthese updates the model can degenerate\ninto a state that it can't recover from\nhere if we allow the probability of the\ntoken out to go all the way up to one it\nmeans we'll never be able to predict\nanything else in the state one way to\ndeal with variants is to constrain the\nmodel updates some algorithms Define a\ntrusted region for these updates here\ntheir effect would be that the ball\nwould not be allowed to leave this\nparameter within a single update this\nidea was introduced by trpo or trusted\nregion policy optimization which defined\nthis area using second order derivatives\npoo adopted the same idea but\nimplemented it via clipping since grpo\nBuilds on top of the PO loss let's first\ndiscuss how the Poo loss diverges from\nthe traditional policy gradient loss in\norder to constrain model updates poo\nlooks at the ratio between the new model\nparameters so after the training step\nand the old ones before the training\nstep we'll denote this ratio by r index\nTheta a ratio of one would mean the\nbefore and after parameters are\nidentical so po constrains this ratio\nbetween 1 minus Epsilon and 1 plus\nEpsilon where Epsilon is a\nhyperparameter between 0 and one so the\nPO loss replaces the log probability\nwith the ratio between the probability\nbefore and after the update clipped\nwithin this interval well almost po\nactually takes the minimum between the\nunclipped versus clipped ratio times the\nadvantage so why is this minimum\nnecessary let's break down what happens\nwhen the advantage is positive versus\nwhen it's negative remember a positive\nadvantage means that the action we took\nwas good and we want to reinforce it\nwhen the ratio R is higher than one it\nmeans the new probability assigned to\nthe action is higher than the older\nprobability so the gradient is moving in\nthe right direction or the direction\nthat agrees with the reward conversely a\nnegative advantage means that the action\nwe took was bad and we want to penalize\nit when the ratio is lower than one the\nnew assigned probability is lower than\nthe older probability so that direction\nof the gradient would indeed penalize\nthe action these regions where the\nadvantage and ratio agree with each\nother are the regions we do want to clip\nto protect against overzealous updates\nthat might be reading too much into the\nspecifics of the trajectory that we're\ncurrently training on on the other hand\nthere will be cases where the ratio\ndisagrees with the advantage on whether\nthe action should be reinforced or\npenalized this can happen due to noise\nor imperfect estimation so in case of\nthis agreement where the gradient is\nalready going in the wrong direction we\nactually do want to penalize it as much\nas possible and this is where the\nunclipped ratio does take effect the\nreason why we spend so much time talking\nabout the Poo loss is because the grpo\nloss uses it directly and simply\nsubtracts an extra term this term\nreferences two policies the one that\nwe're currently optimizing Pi Theta and\na reference policy the reference policy\nis different from the denominator of the\nPO ratio IO where we had Pi old in\ncontrast Pi ref refers to the state of\nthe model before we started the\nreasoning finetuning stage Al together\nso for our original pipeline that would\nbe our preference fine tuned model gpio\ntakes the KL Divergence between the\ncurrent and the reference policy which\nis a sort of distance measure between\nthe two distributions so not only are we\nkeeping the model close to its state\nbefore the current training step by a\nclipping but also close to its original\nstate via this extra loss term\nintuitively this is needed to compensate\nfor the many grpo oversimplifications\nlike removing the state value function\nwhich introduced more variants compared\nto poo putting this together here's what\npoo and grpo do during one training step\nour agent currently in state SD will use\nits policy to produce a probability\ndistribution over actions and then\nsample an action at the environment\nmight give it a reward RT depending on\nthe circumstance this might be the\nintermediate reward or the reward at the\nend of the episode po passes the agent\nState through a value function and then\ncalculates the advantage and finally the\nloss GPO CMS a group of trajectories and\nuses their mean reward as a baseline to\ncompute the advantage then it calculates\nthe PO loss and adds an extra\nregularization term both algorithms will\nthen do gradient Ascent with their\nrespective losses and update the policy\nof the agent so that was prpo in a\nnutshell I know the math can look heavy\nbut at its core the algorithm is quite\nsimple and if there's one idea I want\nyou to take from this is that research\npapers can be a little bit like\nInstagram there's subtle ways to flex\nand Greek symbols are one of them and\nI'm not just blaming deep seek any\nmachine learning engineer or researcher\nhas to play the game in order to get\npublished but once once you see past\nthat these papers become a lot easier to\ndigest if this helped make things clear\nor if made them more confusing I want to\nknow in the comments either way and if\nyou want to keep building your intuition\non reinforcement learning check out my\nprevious po video I'll see you next time"
}